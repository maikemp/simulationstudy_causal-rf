.. _model_specifications:

********************
Model specifications
********************
 
 The directory *src/model_specs* contains four different types of `JSON <http://www.json.org/>`_ files that are required for different tasks:

Simulation Parameters
=====================
* First, there is the file `simulation_parameters.json`. It contains  information on all the items that is iterated over in the `wscript` s. First, a number in "rep_number" indicates how many repetitions the simulation includes. It is chosen very low (2) here, but it can simply be increased causing only the simulations for the additional repetitions to be run. 

	Second, it contains a "list_of_setups" that has to be extended if a new setup is defined (corresponding to two other json files in this folder (`[setup_name]_dgp.json` and `[setup_name]_analysis.json`) as described below). Next, the list "te_only_by_X_0" is defined, where all the setups should be listed where the treatment effect is a function only of the first feature. Only for those setups, it makes sense to plot the treatment effect data, as described below. Next, there is "d_list", containing all the different values for the number of features in the dataset. This list can simply be edited here, leading to the additional samples to be simulated and processed. Last, there is a "list_of_methods" that contains all methods that correspond to a file in *src/analysis/estimate_[file_name].R*. This list should only be edited after making sure the only the values in `[setup_name]_analysis.json` are required for the new method or after adding a new json file for the method specific parameters and adding it to the method specific dependencies in *src/analysis/wscript*.

	For the computationally expensive parts, that are simulating the samples in the data management step and then estimating and predicting with the different methods of the analysis step, the file `simulation_parameters.json` will not be included in the dependencies because that would mean that each time the file is edited, all data would be computed again. The values are simply loaded into the *wscript*s for these steps to iterate over them. Only for the less costly parts, such as stacking together the results and making tables and graphs out of them, the information from the file is directly needed in the files ( *src/analysis/stack_data.R*, *src/final/coverage_tables.R* and *src/final/plot_micro_data.py*) and thus included as a dependency.

	Note, however, that any changes to this file can cause the total computeation time to explode. The number of simulation repetitions, for example, should only be increased after making sure that everythings else works perfectly fine. Otherwise adding, for example a new value in d_list will require a very long time for execution which could be annoying if one only wanted to experiment with new values.

k-NN Parameters
===============
* Second, it contains a short file called `k_list.json` that contains the parameters used in  *src/analysis/estimate_knn.R*. That is, a list called "k_list" and a value for "alpha" that will be used as the significance level for the confidence intervals. They are put into an extra file so that the analyses for other methods don't have to computed again if editing only them. The reason why it is part of the dependency for `estimate_knn.R`, and is not iterated on in the *src/analysis/wscript* is that this method is not that computationally expensive, so I decided to rather recreate all knn data when editing this list over having to create an even larger number of data snippets to stack together later on. The file will also be required in *src/final/coverage_table.R*.

Data Generating Process Parameters
==================================
* The third and fourth type of files in *src/model_specs* need to correspond to a value in the "list_of_setups" in `simulation_parameters.json` such that they are named `[setup_name]_dgp.json` and `[setup_name]_analysis.json`, respectively. 

	The first one defines the data generating process of the setup and is used only in *src/data_management/get_simulated_samples.py*. It contains as "n" the number of observations, and as "x_distr" the distribution. Right now only the values "uniform" and "normal" can be processed for uniform and normal distribution. For other values the file src/data_management/get_simulated_samples.py would have to be edited accordingly. "x_ncorr" will only be relevant if the data is normally distributed and will then indicate the number of mutually correlated variables. It can either be a numerical values (but careful: it cannot be larger than the value for d) or it can have the value "d" indicating that all features are mutually correlated. The values "x_low" and "x_high" are relevant only if the data is uniformly distributed and gives the bounds on the interval the data is distributed over. "sigma" gives the error variance. The last three values correspond to the three functions describing the data generating process: the propensity_function, defining the propensity to receive treatment as a function of the features, the treatment_effect_function defining the true treatment effect as a function of the features, and the base_effect_function defining the effect if no treatment has occurred also as a function of X and of the true treatment effect. The values given here must correspond to a Python module in *src/model_code* containing a function with the same name as the module. I decided to split the multiple functions up into various modules because like this new data generating processes with new functions can be defined without having to generate the other data again. The script *src/data_management/wscript* for each setup automatically defines only the function specified in this json file to its dependency.

Causal Random Forest Parameters
===============================
	The second type contains the parameters that are required for the analysis in *src/analysis/estimate_crf.R*. It gives the "node_size" and the significance level "alpha" as well as a value for "foresttype" that can have the values "double_sample" for using the double sample trees algorithm and "propensity" for using the propensity trees algorithm. In addition, it requires contains values for "n_tree_function" and "sample_size_function". These values can either be a string containing the name of a function defined in *src/model_code/n_tree_functions.R* and *src/model_code/sample_size_functions.R*, respectively or directly give a numerical value for the number of trees in forest and the used sample size, respectively. The script in *scr/analysis/wscript* will only add the R scripts containing the functions to the dependency for the setups, if there is given a function name in this json file. 
