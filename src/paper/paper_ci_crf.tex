\documentclass[11pt, a4paper, leqno]{article}
\usepackage{a4wide}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float, afterpage, rotating, graphicx}
\usepackage{epstopdf}
\usepackage{longtable, booktabs, tabularx}
\usepackage{fancyvrb, moreverb, relsize}
\usepackage{eurosym, calc}
% \usepackage{chngcntr}
\usepackage{amsmath, amssymb, amsfonts, amsthm, bm, MnSymbol}
\usepackage{caption}
\usepackage{mdwlist}
\usepackage{xfrac}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{minibox}
% \usepackage{pdf14} % Enable for Manuscriptcentral -- can't handle pdf 1.5,
% \usepackage{endfloat} % Enable to move tables / figures to the end. Useful for some submissions.



\usepackage{natbib}
\bibliographystyle{rusnat}




\usepackage[unicode=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    citecolor=black,,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=black
}


\widowpenalty=10000
\clubpenalty=10000

\setlength{\parskip}{1ex}
\setlength{\parindent}{0ex}
\setstretch{1.5}


\begin{document}

\title{Simulation Study on the Coverage Probabilities of Confidence Intervals for Heterogeneous Treatment Effects Estimated with Causal Random Forests\thanks{Maike Metz-Peeters, Universit√§t Bonn. Email:gi \href{mailto:maike-m-p@web.de}{\nolinkurl{maike-m-p [at] web [dot] de}}.}}

\author{Maike Metz-Peeters}

\date{
{\bf Preliminary -- please do not quote} 
\\[1ex] 
\today
}

\maketitle


\begin{abstract}
	In this simulation study, I investigate the coverage probabilities of the confidence intervals for treatment effects estimated by a nonparametric causal forest developed by \citet{wa18}. I broadly replicate their results and in addition visualize the relationship between the treatment effect estimator and the true treatment effect for a new experimental setup. I can confirm the author's claim that their estimator performs better in terms of coverage probability and mean squared error than k nearest neighbor matching most of the time. Furthermore, the bias of the estimator seems to be caused by a tendency of the estimator to smoothen bumps in the treatment effect function.
\end{abstract}
\clearpage

\section{Introduction} % (fold)
\label{sec:introduction}

In this paper, I will first give a brief insight into the method itself. Then I will describe the chosen experimental setups for the simulation study and present the results. It follows a short conclusion.


The project was programmed with Waf using a template from \citet{GaudeckerEconProjectTemplates}. 


\section{Background} % (fold)
\label{sec:background}
The method described in \citet{wa18} estimates heterogeneous treatment effects in a highly data-driven way using a special form of random forests while still enabling valid statistical inference. This makes it possible to discover even unexpected structures in the data. In the following I will give a short description of the main idea of this method following \citet{wa18} unless stated otherwise. A detailed description of the method will then be given in master thesis. 

For this purpose, the authors assume having at hand \(n\) i.i.d. draws of a feature vector \(X_i \in [0,1]^d\) or following any other density bounded away from 0 and infinity, a treatment indicator \(W_i \in \{0,1\}\), and an outcome \(Y_i \in \mathbb{R}\).
As usual, the treatment effect is the expectation of the difference between the outcome under treatment and without treatment:
\input{formulas/treatment_effect}
where \(Y_i^{(1)}\) denotes the outcome if treatment has taken place and \(Y_i^{(0)}\) is the outcome without treatment. Heterogeneity in the treatment effect depending on the observable variables is allowed for. 
As usually, unconfoundedness is assumed:
\input{formulas/unconfoundedness}
This means that the selection bias into treatment disappears when controlling for observable features, and thus around observations that are "close" in the feature space behave like a randomized experiment. 
Nearest neighbor methods exploit this assumption. As is described by \citet{lj06}, random forests can also be viewed as an adaptively weighted k potential nearest neighbors method. For this reason, a simple k nearest neighbor matching procedure is the used as baseline method.
As explained in detail by \citet{hir03}, if a consistent estimator for the propensity of receiving treatment \(e(x) = \mathbb{E}[W_i|X_i = x]\)  is available, an unbiased estimator for the treatment effect could be obtained employing: 
\input{formulas/propensity_estimation}

Generally, a random regression forest is build by subsampling (originally by bootstrapping) the data and then growing a tree on each subsample by recursively partitioning the data in order to get the best fit according to some splitting criterion \citep[p.~304 ff.]{htf08}. The partitioning is repeated until some stopping criterion is achieved. The idea is that the data points in the terminal nodes of a trees are similar to each other and therefore a constant estimator as a simple average over the observed outcomes suffices:
\input{formulas/tree_estimator}
where \(L(x)\) denotes the leaf containing \(x\).
The variable to split on is selected from a randomly chosen subset of variables only, in order to decorrelate the individual trees to decrease variance. Since individual trees generally exhibit low bias but large variance, averaging at each data point over the estimators of all subsample-trees, can lead to a substantially improved estimator. 
For a comprehensive overview over this method, see \cite[p.~587 ff.]{htf08}.

To obtain an estimator of heterogeneous treatment effects, \cite{wa18} use the following estimator:
\input{formulas/tau_forest_estimator}

Similarly the nearest neighbor matching estimator is given by: 
\input{formulas/tau_knn_estimator}
where \(S_1\) and \(S_0\) represent the k nearest neighbors in the treatment and control group, respectively.   

To prove pointwise consistency and asymptotic Gaussianity for the random forest estimator, in addition to the above mentioned assumptions \cite{wa18} require overlap and, which means that at each data point, the probabilities to receive and not to receive treatment are strictly positive as well as a property they call "honesty". It means that for each observation, the response can only be used either to select the model (that is to place the splits) or to estimate the treatment effect, but not for both. Furthermore, the conditional expectations of the outcome under treatment and without treatment at any data point have to be Lipschitz continuous.
They show that under these conditions, when using the infinitesimal jackknife developed by \cite{e14} to consistently estimate the variance and under some additional regularity assumptions, the treatment effect estimator from such a random forest is pointwise consistent for the true treatment effect and has an asymptotically Gaussian and centered sampling distribution so that asymptotic confidence intervals can be constructed. 

\cite{wa18} develop two algorithms satisfying honesty: the double-sample trees and the propensity trees. The first one is called "Double sample trees" and it splits the available data into two parts. One part (and the features and treatment indicators of the other half) will be used to place the splits for the tree. The second half will be used to estimate the treatment effect per leaf, using function \ref{eq:tau_crf}.
The second algorithm is called "Propensity Trees" and aims at placing the splits uses only the treatment assignment indicator to place its splits and is especially useful if we there is suspected bias due to a variation in the treatment propensity. For more details on these algorithms please see \cite[p.~1232]{wa18}. A careful description of the evolution of the regression tree method to the method presented here, and a thorough exploration of the causal random forests will be done in my master thesis.


\section{Simulation Study} % (fold)
\label{sec:simulation}

For this project, I investigate four different setups. The first one exactly corresponds to the first setup in \cite{wa18}, while the second and the third setup are less computationally expensive versions of the versions in this paper, that is, I reduced the sample size and the number of trees estimated. It is very easy to change to the exact versions of the paper by simply editing the corresponding Json files in the \textit{src/model\_specs}, but time for computation will increase rapidly. As a fourth setup, I chose a treatment effect function that only depends on \(X_1\), and that has two larger bumps. I did this, to illustrate the claim of \cite[p.~1238]{wa18} that random forest approaches tend to flatten the hills and fill up the valleys of the true treatment effect function.

\input{formulas/setup_table.tex} 

The exact parameter values used for this study are summarized in table  \ref{table:setup_table}. The nominal significance level is 5\%.


\input{../../out/tables/coverage_table_setup_1.tex}\label{tab:setup_1}

The tables \ref{table:setup_1} to \ref{table:setup_4} contain the results of my simulation study. Since they only contain 2 simulation repetitions, they may not be too reliable yet. I chose this small number so the project doesn't take too long to build, and it will increased drastically for the simulation study of my master thesis.
We can see that the results broadly confirm the results in \cite[p.~1238 ff.]{wa18}. Probably because of the low number of simulation runs, the relationship between coverage rate and number of features in setup 1 seems to be less monotonic, but is still similar in order of magnitude. The here used propensity tree algorithm overall achieves good coverage probabilities and, especially compared to the 100 nearest neighbor matching. 

\input{../../out/tables/coverage_table_setup_2.tex}

For setup 2, the results are not quite as good as in the \cite{wa18} which may be due to the lower sample size. Here, the algorithm seems to struggle when larger numbers of features are taken into account. However, The double sample algorithm still outperforms the k nearest neighbor matching, especially in terms of mean squared error. 

\input{../../out/tables/coverage_table_setup_3.tex}

The third setup has a sharper treatment effect function. Coverage rates and mean squared errors are therefore expected to worsen a bit since more bias is expected. This can not unambiguously be confirmed by my replication. To further investigate this, I will use more simulation repetitions in my master thesis.

\input{../../out/tables/coverage_table_setup_4.tex}

The fourth setup was chosen to be able to plot the true treatment effect and the estimated treatment effect against each other to look into when the bias is especially striking. One can already in table \ref{table:setup_4} that coverage rates are not close to the nominal coverage rate anymore and that the situation gets drastically worse as the number of features increases. Only the 10 nearest neighbor algorithm still exhibits somewhat acceptable performance. If we look at the plots in figure \ref{fig:setup_4}, we get an idea how for the reason.

\begin{figure}
    \caption{Plot of Micro data for one forest and one testset of Setup 4 with d=3}
    
    \includegraphics[width=\textwidth]{../../out/figures/te_plot_setup_4}
    \label{fig:setup_4}

\end{figure}

The black function shows the true treatment effect, while the dots are the estimated treatment effects and the "whiskers" indicate the width of the estimated confidence intervals. The points and its whiskers are shown in green if the interval covers the true treatment effect and in red if it fails to do so. It seems that while for only 2 features in the sample, only the estimators in regions where the treatment effect function is especially steep are off more often while the extreme points still seem to be covered quite well. 

But as the number of features increases, it seems that the treatment effect function is "smoothened" more and more up to a situation where all estimators show approximately equal values, that are somewhere around the average treatment effect while the hills and valleys are hardly represented anymore.
To investigate the reasons of this behavior and look into improvements for these problems will be part of my master thesis.

\section{Conclusion} % (fold)
\label{sec:conclusion}

To conclude, I can overall confirm the results by \cite{wa18} that the causal random forests are in some situations suitable for estimation of heterogeneous treatment effects and valid inference for them. Bumpy treatment effect functions and larger numbers of features, however, seem to lead to substantial bias. 

Getting deeper into the reasons for this behavior and looking into possible solutions for this problem will be part of my master thesis. There will also be given a much deeper insight into the rationale behind the method. Looking into how richer data structures, that is correlated variables and more complex main effect functions, will also be done there. 

\pagebreak





\bibliography{refs}



% \appendix

% The chngctr package is needed for the following lines.
% \counterwithin{table}{section}
% \counterwithin{figure}{section}

\end{document}
